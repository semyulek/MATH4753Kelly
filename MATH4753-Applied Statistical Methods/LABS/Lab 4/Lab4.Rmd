---
title: "Lab4"
author: "Samuel Kelly"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Task 1

## Get Working Directory
```{r}
getwd()
```

# Task 2

## Read SPRUCE.csv
Read the .csv file and output the tail.
```{r}
spruce.df <- read.table("SPRUCE.csv", header = T, sep = ",")
tail(spruce.df)
```

# Task 3
## Trendscatter plot
Make a trendscatter plot with f=.5, of Height(m) vs BHDiameter(cm).
```{r}
library(s20x)
trendscatter(Height~BHDiameter,data = spruce.df,f = .5)
```

## Linear Model
Make the spruce linear model.
```{r}
spruce.lm = with(spruce.df, lm(Height~BHDiameter))
```

## Find Residuals
```{r}
height.res = residuals(spruce.lm)
```

## Find Fitted
```{r}
height.fit = fitted(spruce.lm)
```

## Plot Residuals vs. Fitted
```{r}
plot(height.res~height.fit)
```

## Plot Residuals vs. Fitted Using Trendscatter
```{r}
trendscatter(height.fit,height.res)
```

## What shape?
The trendscatter of residuals vs fitted clearly shows there is a curve. This curve is also observed in the first trendscatter plot that we plotted above. Side-by-side comparison.
```{r}
layout(matrix(1:2,nrow = 2,ncol = 1,byrow = F))
trendscatter(Height~BHDiameter,data = spruce.df,f = .5)
trendscatter(height.fit,height.res)
```

## Residual Plot
The linear model automatically prints out 4 different plots, so we must specify which one we want to use.
```{r}
plot(spruce.lm, which = 1)
```

## Normality
```{r}
normcheck(spruce.lm,shapiro.wilk = T)
```

## P-value, NULL Hypothesis
We should not reject the null hypothesis, since our p-value is .29, then our bell curve is distributed normally. P-value of .29 is not significant.

## Validity
To validate our claim then we must see that the height residuals mean equals 0. 
```{r}
round(mean(height.res),digits = 4)
```
When we look at the residuals vs fitted graph, we can see that the data curves and is not straight, so we should use a quadratic, since we need a curve with concavity.

# Task 4

## Quadratic
Square the x-value or BHDiameter, and leave it as is.
```{r}
quad.lm <- lm(Height~BHDiameter + I(BHDiameter^2),data = spruce.df)
plot(spruce.df$BHDiameter, spruce.df$Height, main = "Spruce Height vs. BHDiameter Prediction",
     xlab = "BHDiameter(cm)", ylab = "Height(m)", pch = 21, bg = "Blue", cex = 1.2,
     ylim = c(0, 1.1 * max(spruce.df$Height)), xlim = c(0, 1.1 * max(spruce.df$BHDiameter)))
```

## Add curve
```{r}
coef(quad.lm)
myplot = function(x){
  0.86089580 + 1.46959217*x -0.02745726*x^2
}
plot(spruce.df$BHDiameter, spruce.df$Height, main = "Spruce Height vs. BHDiameter Prediction",
     xlab = "BHDiameter(cm)", ylab = "Height(m)", pch = 21, bg = "Blue", cex = 1.2)
curve(myplot, lwd = 2, col = "steelblue", add = T)
```

## Alternative formula for calculating quadratic in R
```{r}
myplot2 = function(x){
  quad.lm$coef[1] + quad.lm$coef[2]*x + quad.lm$coef[3]*x^2
}
plot(spruce.df, main = "Spruce Height vs. BHDiameter Prediction",
     xlab = "BHDiameter(cm)", ylab = "Height(m)", pch = 21, bg = "Blue", cex = 1.2)
curve(myplot2, lwd = 2, col = "steelblue", add = T)
```

## Quad Fit
```{r}
quad.fit = fitted(quad.lm)
```

## Quadratic Residual vs Fitted
```{r}
plot(quad.lm, which = 1)
```

## Normcheck
```{r}
normcheck(quad.lm, shapiro.wilk = T)
```

## Validity
The P-value is .684 which suggests that we accept the null hypothesis, so the data is normally distributed.
We can conclude that the quadratic model fits better than the linear model we used in lab 3.

# Task 5

## Summary
```{r}
summary(quad.lm)
```

## Beta values
$$\hat{\beta}_{0} = 0.860896$$
$$\hat{\beta}_{1} = 1.469592 $$
$$\hat{\beta}_{2} = -0.027457$$

## Interval Beta estimates
This shows the z-scores for the upper and lower limits with a 95% accuracy.
```{r}
ciReg(quad.lm)
```

## Fitted Line Equation
$$Height = 0.860896 + 1.469592*x + (-0.027457)*x^2$$

## Prediction
```{r}
predict(quad.lm, data.frame(BHDiameter = c(15, 18, 20)))
```

## Comparison
```{r}
predict(spruce.lm, data.frame(BHDiameter = c(15, 18, 20)))
```
The values are overall larger in the quadratic prediction.

## R-Squared
```{r}
summary(spruce.lm)$r.squared
summary(quad.lm)$r.squared
```

## Adjusted R-Squared
```{r}
summary(spruce.lm)$adj.r.squared
summary(quad.lm)$adj.r.squared
```
The adjusted R-squared for the quadratic is higher than the linear model. Adjusted R-squared is a sort of accountability for adding too many variables that might have a chance correlation with your data, thus adjusted R-squared will decrease the more variables are added which do not correlate. Since our adjusted comparison for the quadratic is higher than the linear, we should use the quadratic.

## Multiple R-Squared
The multiple R-squared describes how good the models fit with the data.

## Variability
The higher the R-squared value that we can use can explain the variance in the data.
```{r}
summary(spruce.lm)$r.squared
summary(spruce.lm)$adj.r.squared
summary(quad.lm)$r.squared
summary(quad.lm)$adj.r.squared
```
Since the quadratic model is greater than the linear model, then it can explain the variance in height better than the linear model.

## Anova comparison
```{r}
anova(spruce.lm, quad.lm)
```
The smaller the residual sum of squares is, the better the data fits with the chosen model. In this case the quadratic model fits best with the data.

## RSS, MSS, TSS
```{r}
height.qfit = fitted(quad.lm)
(RSS = with(spruce.df, sum((Height - height.qfit)^2)))
(MSS = with(spruce.df, sum((height.qfit - mean(Height))^2)))
(TSS = with(spruce.df, sum((Height - mean(Height))^2)))
MSS/TSS
summary(quad.lm)$r.squared
```

# Task 6

## Cooks
```{r}
cooks20x(quad.lm)
```

## Cook's Influence
Cook's plot is mainly used to identify outliers/influencers on the data set. Here it shows that data value 24 has the highest influence with 18 and 21 following after. If 24 is removed the R-squared values should increase, since 24 seems to be an outlier in the data.

## Removing 24
```{r}
quad2.lm <- lm(data = spruce.df[-24,],formula = Height~BHDiameter + I(BHDiameter^2))
```

## Summary
```{r}
summary(quad.lm)
summary(quad2.lm)
```
The multiple R-squared value has increased after removing the datum 24. The residual values have decreased which means that are quadratic model better fits with the data.

## Conclusion
Cook's plot showed that datum 24 had largest effect on the data set. After removing 24 from the data, we see that the multiple R-squared value has increased, so removing 24 has had a positive effect on our data.

# Task 7

## Proof Piece-wise Regression

$$
Prove \: that \:
$$
$$
\begin{equation} y = \beta_{0} + \beta_{1}x + \beta_{2}(x - x_{k})I(x > x_{k}) \end{equation}
$$
$$
\: where \: I() \: is \: 1 \: when \: x>x_{k} \:and \:0 \:else.
$$

$$\\\\$$

$$
Suppose \:that \:l_{1}:\:y=\beta_{0} +\beta_{1}x \:\:and \: \: l_{2}:\: y=\beta_{0} + \delta +(\beta_{1} +\beta_{2})x
$$
$$
so:\: y_{k}=\beta_{0} +\beta_{1}x_{k} = \beta_{0} + \delta +(\beta_{1} +\beta_{2})x_{k}
$$
$$
Distribute \:x_{k}:  \beta_{0} +\beta_{1}x_{k} = \beta_{0} + \delta + \beta_{1}x_{k} +\beta_{2}x_{k}
$$ 
$$
Then \: subtract \: \beta_{0} +\beta_{1}x_{k} \: from \: both \: sides.
$$
$$
0 = \delta+\beta_{2}x_{k} \Rightarrow\delta=-\beta_{2}x_{k}
$$
$$
Now \: that \:we \:have\: \delta\: substitute\: \delta\: back\: into\: l_{2}
$$ 
$$
l_{2}:y=\beta_{0} + \delta +(\beta_{1} +\beta_{2})x\Rightarrow y=\beta_{0}  -\beta_{2}x_{k} +(\beta_{1} +\beta_{2})x
$$
$$
Distribute:y=\beta_{0}-\beta_{2}x_{k}+\beta_{1}x+\beta_{2}x
$$
$$
Group:y=\beta_{0}+\beta_{1}x+\beta_{2}x-\beta_{2}x_{k}
$$
$$
Factor \:out \: \beta_{2}:y=\beta_{0}+\beta_{1}x+(x-x_{k})\beta_{2}
$$
$$
Therefor:y = \beta_{0} + \beta_{1}x + \beta_{2}(x - x_{k})I(x > x_{k}) \: when \:x > x_{k} 
\:then \: I() \: is, \: else \: 0.
$$

## Reproduce Plot

